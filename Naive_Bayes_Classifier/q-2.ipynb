{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"LoanDataset/data.csv\")\n",
    "df.columns = ['id','age','exp','income','zip','family','spend','education','house','output','security','certi','net','creditcard']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['output','zip','id'],axis=1)\n",
    "Y = df['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([X_train,Y_train],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = ['age','education','exp','income','family','spend','house','output']\n",
    "categorical = ['security','certi','net','creditcard','output']\n",
    "numericdata = df[numeric]\n",
    "catdata = df[categorical]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct = {}\n",
    "def seperateClass(numericdata):\n",
    "    for index,row in numericdata.iterrows():\n",
    "\n",
    "        if row['output'] not in distinct:\n",
    "            distinct[row['output']]=[]\n",
    "        distinct[row['output']].append(list(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(attrColumn):\n",
    "    return float(sum(attrColumn))/len(attrColumn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_deviation(attrColumn):\n",
    "    avg = mean(attrColumn)\n",
    "    ans = 0\n",
    "    for i in attrColumn:\n",
    "        ans=ans+pow(i-avg,2)\n",
    "    ans = float(ans)/(len(attrColumn)-1)\n",
    "    return np.sqrt(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mean_sd_for_attr(numericdata):\n",
    "\n",
    "    summaries = {}\n",
    "    df = pd.DataFrame(numericdata)\n",
    "    df.columns = numeric\n",
    "    for i in df.columns:\n",
    "        summaries[i]=(mean(df[i]),standard_deviation(df[i]))\n",
    "    del summaries['output']\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [],
   "source": [
    "seperateClass(numericdata)\n",
    "# print distinct\n",
    "summary = {}\n",
    "for label,data in distinct.items():\n",
    "    summary[label] = find_mean_sd_for_attr(data)\n",
    "    \n",
    "# print summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarycat = {}\n",
    "for j in catdata['output'].unique():\n",
    "            summarycat[j] = []\n",
    "def find_summary_category(catdata,o):\n",
    "    temp = {}\n",
    "    for j in categorical:\n",
    "        temp1={}\n",
    "        if j == 'output':\n",
    "            continue\n",
    "        for i in catdata[j].unique():\n",
    "            num = len(catdata[j][catdata[j] == i][catdata['output'] == o])\n",
    "            den = len(catdata[j][catdata['output'] == o])\n",
    "            temp1[i]=num/den\n",
    "        temp[j] = temp1\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'security': {0: 0.8992319508448541, 1: 0.10076804915514594},\n",
       "  'certi': {0: 0.9640552995391705, 1: 0.035944700460829496},\n",
       "  'net': {1: 0.6021505376344086, 0: 0.3978494623655914},\n",
       "  'creditcard': {1: 0.2924731182795699, 0: 0.7075268817204301}},\n",
       " 1: {'security': {0: 0.877906976744186, 1: 0.12209302325581395},\n",
       "  'certi': {0: 0.7151162790697675, 1: 0.28488372093023256},\n",
       "  'net': {1: 0.6191860465116279, 0: 0.3808139534883721},\n",
       "  'creditcard': {1: 0.29941860465116277, 0: 0.7005813953488372}}}"
      ]
     },
     "execution_count": 848,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in catdata['output'].unique():\n",
    "    summarycat[i]=find_summary_category(catdata,i)\n",
    "summarycat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcProbability(x,mean,sd):\n",
    "    exponent = math.exp(-(math.pow(x-mean,2)/(2*math.pow(sd,2))))\n",
    "    return (1 / (math.sqrt(2*math.pi) * sd)) * exponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_by_class(inputdata,summary,summarycat):\n",
    "\n",
    "    probabilities = {}\n",
    "    for i in catdata['output'].unique():\n",
    "        probabilities[i]=len(catdata['output'][catdata['output'] == i])/len(catdata)\n",
    "                               \n",
    "    for label, labeldata in summary.items():\n",
    "\n",
    "        for key,value in labeldata.items():\n",
    "\n",
    "            mean,sd = value\n",
    "            inputattrdata = inputdata[key]\n",
    "            probabilities[label] *= calcProbability(inputattrdata, mean, sd)\n",
    "            \n",
    "    for label, labeldata in summarycat.items():\n",
    "        for key,value in labeldata.items():\n",
    "            inputattrdata = inputdata[key]\n",
    "            probabilities[label] *= summarycat[label][key][inputattrdata]\n",
    "            \n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(inputdata,summary,summarycat):\n",
    "    probabilities = probability_by_class(inputdata,summary,summarycat)\n",
    "    anslabel = \"\"\n",
    "    maxprob = 0\n",
    "    for label, probability in probabilities.items():\n",
    "        if anslabel==\"\" or probability >= maxprob:\n",
    "            maxprob = probability\n",
    "            anslabel = label\n",
    "#     print anslabel\n",
    "    return anslabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[772  38]\n",
      " [ 35  55]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95       810\n",
      "           1       0.59      0.61      0.60        90\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       900\n",
      "   macro avg       0.77      0.78      0.78       900\n",
      "weighted avg       0.92      0.92      0.92       900\n",
      "\n",
      "91.88888888888889\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "\n",
    "for index,row in X_test.iterrows():\n",
    "    res.append(prediction(row,summary,summarycat))\n",
    "\n",
    "print(confusion_matrix(Y_test,res))\n",
    "print(classification_report(Y_test,res))\n",
    "print(accuracy_score(Y_test, res)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[754  56]\n",
      " [ 42  48]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94       810\n",
      "           1       0.46      0.53      0.49        90\n",
      "\n",
      "   micro avg       0.89      0.89      0.89       900\n",
      "   macro avg       0.70      0.73      0.72       900\n",
      "weighted avg       0.90      0.89      0.89       900\n",
      "\n",
      "89.11111111111111\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, Y_train)\n",
    "y_pred = gnb.predict(X_test)\n",
    "print(confusion_matrix(Y_test,y_pred))\n",
    "print(classification_report(Y_test,y_pred))\n",
    "print(accuracy_score(Y_test, y_pred)*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
