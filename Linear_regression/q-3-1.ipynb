{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"AdmissionDataset/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Chance of Admit ','Serial No.'],axis=1)\n",
    "Y = df['Chance of Admit ']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.2)\n",
    "# Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7716858889215062\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, Y_train) \n",
    "y_pred = lr.predict(X_test)\n",
    "print(r2_score(Y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = (X_train - X_train.mean())/X_train.std()\n",
    "# Y_train = (Y_train - Y_train.mean())/Y_train.std()\n",
    "X_test = (X_test - X_test.mean())/X_test.std()\n",
    "# Y_test = (Y_test - Y_test.mean())/Y_test.std()\n",
    "# X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train,Y_train],axis=1)\n",
    "ones = np.ones([X_train.shape[0],1])\n",
    "Y_train = X_train.iloc[:,7:8].values\n",
    "X_train = X_train.iloc[:,0:7]\n",
    "X_train = np.concatenate((ones,X_train),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "iterations = 1000\n",
    "# theta = np.zeros(8) # 7 is the number of features\n",
    "theta = np.zeros([1,8])\n",
    "# theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_decent(X_train,Y_train,theta,learning_rate,iterations):\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        theta = theta - (learning_rate/len(X_train)) * np.sum(X_train * (X_train @ theta.T - Y_train), axis=0)\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = gradient_decent(X_train,Y_train,theta,learning_rate,iterations)\n",
    "g = g[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for index,rows in X_test.iterrows():\n",
    "    y = 0\n",
    "    rows = list(rows)\n",
    "    for i in range(len(rows)):\n",
    "        y = y + rows[i]*g[i+1]\n",
    "    y = y + g[0]\n",
    "    y_pred.append(y)\n",
    "\n",
    "# Vectorized form\n",
    "# ones = np.ones([X_test.shape[0],1])\n",
    "# X_test = np.concatenate((X_test,ones),axis=1)\n",
    "# y_pred = X_test @ g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8337595544619549,\n",
       " 0.6688428593604098,\n",
       " 0.7615746823168031,\n",
       " 0.7940392451223626,\n",
       " 0.528242267922578,\n",
       " 0.671810676133628,\n",
       " 0.7966787283660897,\n",
       " 0.8462139646738776,\n",
       " 0.666728258055749,\n",
       " 0.7351361981087097,\n",
       " 0.9917951841982863,\n",
       " 0.5916059692412268,\n",
       " 0.8465374885184445,\n",
       " 0.7572022590673759,\n",
       " 0.9357817679766806,\n",
       " 0.8568925302348894,\n",
       " 0.6047976872983482,\n",
       " 0.6270551697048496,\n",
       " 0.5311785943553353,\n",
       " 0.8515839560144198,\n",
       " 0.5397913907748612,\n",
       " 0.7475999663379962,\n",
       " 0.7100181130081107,\n",
       " 0.7247217101869076,\n",
       " 0.6472187334768703,\n",
       " 0.9019371767865533,\n",
       " 0.6701481529396375,\n",
       " 0.7077264855622217,\n",
       " 0.8642846381800091,\n",
       " 0.571036327980919,\n",
       " 0.940498062400348,\n",
       " 0.8061189696541649,\n",
       " 0.6142844723260292,\n",
       " 0.6087063093825105,\n",
       " 0.7588773142840705,\n",
       " 0.517733815436491,\n",
       " 0.8120788994953645,\n",
       " 0.5441223852139148,\n",
       " 0.6911770926068821,\n",
       " 0.4798657138450585,\n",
       " 0.6517857442379476,\n",
       " 0.9775434881761367,\n",
       " 0.7971312944894172,\n",
       " 1.0014470803148825,\n",
       " 0.7428273951495431,\n",
       " 0.751383968432724,\n",
       " 0.8320324397338096,\n",
       " 0.6777659920965253,\n",
       " 0.7914154009044595,\n",
       " 0.65824024830079,\n",
       " 0.6973185119372582,\n",
       " 0.658976697239692,\n",
       " 0.6149910119492858,\n",
       " 0.7502930022893043,\n",
       " 0.6246086500223295,\n",
       " 0.6397938378017812,\n",
       " 0.6641479635791727,\n",
       " 0.6477936853067147,\n",
       " 0.7231416804969427,\n",
       " 0.6454353303601732,\n",
       " 0.478388044070155,\n",
       " 0.6319445867397653,\n",
       " 0.6376714696986507,\n",
       " 0.8218916402804002,\n",
       " 0.8236730487412957,\n",
       " 0.6544007035306851,\n",
       " 0.7551227237686124,\n",
       " 0.9410727443507364,\n",
       " 0.5574303181707951,\n",
       " 0.9566010685563786,\n",
       " 0.9701986591274145,\n",
       " 0.5673699794861545,\n",
       " 0.7936792639074355,\n",
       " 0.6604689412806483,\n",
       " 0.8113101602157452,\n",
       " 0.663384315426081,\n",
       " 0.6667094538452121,\n",
       " 0.7799036441902211,\n",
       " 0.7486197862066849,\n",
       " 0.5043935496220076,\n",
       " 0.683313487235263,\n",
       " 0.6143396932132893,\n",
       " 0.9476470713913766,\n",
       " 0.8254633663388027,\n",
       " 0.9887069359743716,\n",
       " 0.6380262046124469,\n",
       " 0.7048834151152078,\n",
       " 0.7043545595587238,\n",
       " 0.6324271568123334,\n",
       " 0.6553162872156044]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7604522083202787\n"
     ]
    }
   ],
   "source": [
    "print(r2_score(Y_test,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
